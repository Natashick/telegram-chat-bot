ich will einen telegram chat-bot erstellen, der laut meinen dateien auf die nutzerfragen antwortet wird. die idee ist: mit Azure microsoft es erledigen.  hier ist eine zusammenfasung, was wir gestern gemacht haben: Ich habe einen container "pdfchatbots"  auf Azure erstellt mit einen ordner wo meine pdfs sich befinden (ISO-SAE_21434.pdf, UN_Regulation155.pdf). und danach habe ich speicherplatz da erstellt. ich habe lockal in visual code studio mit python skripten vorbereitet: 
# .env
HF_API_TOKEN=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
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=pdfchatbotstorage;AccountKey=...;EndpointSuffix=core.windows.net
AZURE_BLOB_CONTAINER=pdfs

#gitignore

.env
__pycache__/

# app.py
# cd "C:\Users\Student\OneDrive - orhunsuzer.com\Desktop\Praktikum\Mein Praktikum\pdf_chatbot_backend
# curl -X POST http://localhost:5000/ask -H "Content-Type: application/json" -d "{\"question\": \"What is ISO 21434 about?\"}"

from flask import Flask, request, jsonify
from dotenv import load_dotenv
import os
import requests
from pdf_utils import read_all_pdfs_from_blob

# Lade Umgebungsvariablen aus .env
load_dotenv()

# Erstelle Flask-App
app = Flask(__name__)

# Hugging Face API-Key aus .env
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
if not HF_API_TOKEN:
    raise EnvironmentError("HF_API_TOKEN not found. Add it to your .env file.")

# Hugging Face Modell-URL
HF_API_URL = "https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-6-llama-30b"

# Headers vorbereiten
HEADERS = {
    "Authorization": f"Bearer {HF_API_TOKEN}",
    "Content-Type": "application/json"
}

@app.route('/ask', methods=['POST'])
def ask():
    try:
        # Eingehende JSON-Daten abrufen
        data = request.get_json(force=True)
        question = data.get("question", "").strip()

        if not question:
            return jsonify({"error": "No question provided."}), 400

        # Kontext aus PDFs lesen
        context = read_all_pdfs_from_blob()
        context_chunk = context[:800]

        # Prompt bauen
        prompt = (
            f"Answer the question based on the context below.\n\n"
            f"Context:\n{context_chunk}\n\n"
            f"Question: {question}\nAnswer:"
        )

        # Anfrage an Hugging Face senden
        response = requests.post(
            HF_API_URL,
            headers={
                "Authorization": f"Bearer {HF_API_TOKEN}",
                "Content-Type": "application/json"
            },
            json={"inputs": prompt}
        )

        # Fehlerbehandlung
        if response.status_code != 200:
            return jsonify({
                "error": "Hugging Face API error",
                "details": response.json()
            }), response.status_code

        result = response.json()
        return jsonify({"answer": result.get("generated_text", result)})

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# App starten
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)

#Dockerfile
# Verwende ein sicheres Basis-Image
FROM python:3.10-slim-bullseye

# Setze Arbeitsverzeichnis
WORKDIR /app

# Installiere Systemupdates (gegen SicherheitslÃ¼cken)
RUN apt-get update && apt-get upgrade -y && apt-get clean

# Kopiere requirements.txt und installiere Python-AbhÃ¤ngigkeiten
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Kopiere restliche Projektdateien
COPY . .

# Starte die App (kannst du beim Deployment Ã¼berschreiben)
CMD ["python", "app.py"]


# pdf_utils.py
import os
import PyPDF2
from azure.storage.blob import BlobServiceClient
from io import BytesIO

def read_all_pdfs_from_blob():
    connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
    container_name = os.getenv("AZURE_BLOB_CONTAINER")

    if not connection_string or not container_name:
        raise ValueError("Azure connection string or container name is missing in .env")

    blob_service_client = BlobServiceClient.from_connection_string(connection_string)
    container_client = blob_service_client.get_container_client(container_name)

    all_text = ""

    for blob in container_client.list_blobs():
        if blob.name.endswith(".pdf"):
            blob_client = container_client.get_blob_client(blob)
            stream = BytesIO(blob_client.download_blob().readall())
            reader = PyPDF2.PdfReader(stream)
            for page in reader.pages:
                all_text += page.extract_text() or ""

    return all_text

# requirements.txt
flask
azure-storage-blob
PyPDF2
requests
python-dotenv.






1. Architekturâ€‘Entwurf & Technologieâ€‘Stack

Zielarchitektur: Retrievalâ€‘Augmented Generation (RAG)

    Dokumentâ€‘Ingestion: PDFs in Azure Blob (wie duâ€™s hast).

    Dokumentvorverarbeitung: PDF-Chunks â†’ Embeddings â†’ Vektorspeicher.

    Inference-Endpunkt: LLM (â€žHugging Faceâ€œ, OpenAI oder lokal via HF TGI) + Retrieval Chain (z.â€¯B. LangChain).

    Telegramâ€‘Botâ€‘Frontend: Liest Userâ€‘Fragen, ruft RAG-Service auf, sendet Antworten.

ðŸ” Best Practice: Cloudâ€‘gehostete Retrievalâ€‘Pipelines (z.â€¯B. Azure Machine Learning RAG) automatisieren Chunking/Embedding und bieten Skalierbarkeit & Monitoring
Medium+2Algorhythm+2TECHCOMMUNITY.MICROSOFT.COM+2
Hugging Face+1Google Cloud+1
Reddit
GitHub+8Medium+8Medium+8
.
2. Dokumentvorbereitung mit LangChain

Nutze LangChain-Loader fÃ¼r Azure Blob:

from langchain_community.document_loaders import AzureBlobStorageContainerLoader
loader = AzureBlobStorageContainerLoader(conn_str=..., container="pdfs")
docs = loader.load()

Verwende Parser wie PDFPlumberLoader oder PyPDFLoader fÃ¼r sauberes Chunking
Hugging Face
.
3. Embeddings & Vektorspeicher

WÃ¤hle Modell (z.â€¯B. sentence-transformers/all-MiniLM-L6-v2) fÃ¼r semantische Suche â†’ Embeddings â†’ Vektorset (FAISS, Chroma o.â€¯Ã¤.).

Optionen:

    Eigenbau mit LangChain und FAISS lokal.

    Azure ML RAG Pipeline: Automatisches Chunking + Embedding + Vektor-Index-Aufbau in managed FAISS
    Omi AI+9Algorhythm+9blog.fabric.microsoft.com+9
    Reddit
    .

4. LLM-Endpunkt bereitstellen

EinstiegsmÃ¶glichkeiten bei Azure:
Option	Beschreibung
Azure ML + HuggingFace Model Registry	Schnelle Bereitstellung per GUI oder Python SDK
Medium+4Microsoft Learn+4Medium+4
Azure Container Apps / ACI + HF TGI	Selbst gehostet: z.â€¯B. TGI Container auf ACI (mit >4â€¯GB RAM/2 CPU)

Vorgehen mit Azure ML + HuggingFace:

    Modell wÃ¤hlen (z.â€¯B. OpenAssistant/oasst-sft-6-llama-30b).

    Online Endpoint erstellen â€“ CPU/GPU, Auto-Scaling
    Google Cloud+15Microsoft Learn+15Microsoft Azure+15
    .

    REST-Schnittstelle zum AusfÃ¼hren von Prompts bereitstellen.

Alternative (selbst gehostet):

    Nutze TGIâ€‘Container mit Azure Container Apps oder AKS
    Reddit+13Medium+13GitHub+13
    .

5. RAG Chatbot Service implementieren

Erstelle einen Pythonâ€‘Service (Flask/FastAPI):

    Eingabe: Userâ€‘Frage + relevanteste Chunks via ANN-Suche.

    Prompt-Template:

    Kontext: {retrieved_chunks}
    Frage: {question}
    Antwort:

    Anfrage an LLM-Endpoint.

    Antwort zurÃ¼ck an Telegram.

Vermeide, alle PDFs direkt ins Prompt zu laden â€“ brich auf ~1k Tokens auf.
6. Telegramâ€‘Bot Setup

Erzeuge Bot bei BotFather, speichere Token.

In Python (mit python-telegram-bot):

from telegram import Update
from telegram.ext import Updater, CommandHandler, MessageHandler, Filters

updater = Updater(token=TELEGRAM_TOKEN)
dp = updater.dispatcher

def handle_msg(update, context):
    q = update.message.text
    # send to RAG-Service, dann reply

dp.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_msg))
updater.start_polling()

â†’ Fragen an deinen lokalen/Cloudâ€‘RAGâ€‘Service weiterleiten.
7. Containerisierung ðŸ’¡

Baue Dockerâ€‘Image:

    Installiere Dependencies, LangChain, HFâ€‘Client, azureâ€‘blob.

    In Dockerfile: Service + Telegramâ€‘Bot starten.

    Achte auf multi-stage builds und minimale Basisâ€‘Images.

8. Deployment & CI/CD

    Use GitHub Actions (Azure login via Service Principal)
    GitHub
    python.langchain.com+10Hugging Face+10YouTube+10
    Omi AI+3Microsoft Learn+3Medium+3
    Google Cloud
    Hugging Face
    GitHub
    .

    Deploy auf Azure Container Apps oder Azure Web App for Containers.

    Sicheres Management via Azure Key Vault fÃ¼r Tokens/Keys.

    Ãœberwache CPU, Speicherauslastung und Latenzen.

9. Sicherheit & Datenschutz

    Authentifiziere RessourcenToken sicher (Key Vault, Managed Identities).

    PrÃ¼fe Token-Permissions (Blobâ€‘Lesen, Key Vaultâ€‘Zugriff).

    Rotationspolicy fÃ¼r Secrets.

    Logge Anfragen & Antworten (ggf. anonymisiert).

10. Monitoring & Skalierung

    Nutze Azure Monitor, Logging in App Insights.

    Leite Alerts ein (z.â€¯B. Latenz Ã¼ber 2â€¯s, Fehlerraten).

    Auto-Scaling fÃ¼r LLM-Endpoint & RAG-Service definieren.

11. Testing & QualitÃ¤tssicherung

    Unit-/Integrationstests (Telegram Handler, RAG-Chain).

    Stagingâ€‘Endpoint bereitstellen.

    Loadâ€‘Tests mit Tools wie Locust oder JMeter.

ðŸ”œ NÃ¤chste konkrete Schritte:

    Entscheide: Azure-HuggingFace-Endpoint vs. eigener TGI-Container.

    Implementiere LangChain-Laden & Chunking local, teste Retrieval+Embedding.

    Entwickle RAG-Service (FastAPI) und teste lokal mit Beispielprompts.

    ErgÃ¤nze Telegram-Handler â†’ alles lokal testen.

    Container bauen â†’ deployen.

    CI/CD-Workflow bauen â†’ Monitoring aktivieren â†’ in Produktion bringen.

âœ” Zusammenfassung:

    RAG-Architektur ist bewÃ¤hrte Cleanâ€‘Separation von Docs, Retrieval & LLM.

    Azure ML erleichtert Deployment mit Managed Endpoints.

    LangChain bietet robuste Document-/Loader-Tools fÃ¼r source-chunks.

    Containerisierung + CI/CD + Monitoring macht alles stabil & skalierbar.

Sag Bescheid, wenn du Codeâ€‘Snippets oder konkrete Tutorials brauchst â€“ ich helf dir Schritt fÃ¼r Schritt!