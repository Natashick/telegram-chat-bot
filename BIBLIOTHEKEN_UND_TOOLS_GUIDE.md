# üìö KOMPLETTER GUIDE: BIBLIOTHEKEN, TOOLS & ORDNER

## üéØ WAS IST DIESER GUIDE?
Dieser Guide erkl√§rt **JEDE** Bibliothek, **JEDES** Tool und **JEDEN** Ordner in dem Telegram Bot Projekt.
**LERNZIEL:** Du verstehst warum wir was verwenden und wie alles zusammenarbeitet.

---

## üì¶ PYTHON BIBLIOTHEKEN (requirements.txt)

### 1. **TELEGRAM BOT BIBLIOTHEKEN**

#### `python-telegram-bot==20.6`
- **WAS:** Offizielle Python-Bibliothek f√ºr Telegram Bots
- **WARUM:** Macht Telegram API einfach benutzbar
- **OHNE DAS:** M√ºsstest du HTTP-Requests manuell schreiben
- **VERWENDET IN:** `bot.py`, `handlers.py`
- **BEISPIEL:** `await update.message.reply_text("Hallo!")`

---

### 2. **WEB-SERVER BIBLIOTHEKEN**

#### `fastapi==0.104.1`
- **WAS:** Modernes Python Web-Framework
- **WARUM:** Erstellt HTTP-Server f√ºr Telegram Webhooks
- **OHNE DAS:** Bot k√∂nnte nur Polling (langsam), nicht Webhooks (schnell)
- **VERWENDET IN:** `bot.py`
- **ALTERNATIVE:** Flask, Django (komplizierter)

#### `uvicorn==0.24.0`
- **WAS:** ASGI Server (l√§uft FastAPI)
- **WARUM:** FastAPI braucht einen Server um zu laufen
- **OHNE DAS:** FastAPI kann nicht starten
- **KOMMANDO:** `uvicorn bot:app --host 0.0.0.0 --port 8000`

#### `aiohttp==3.9.0`
- **WAS:** Asynchroner HTTP Client
- **WARUM:** F√ºr HTTP-Requests an Ollama LLM Server
- **OHNE DAS:** Bot kann nicht mit Ollama kommunizieren
- **VERWENDET IN:** `llm_client.py`, `vector_store.py`
- **VORTEIL:** Async = non-blocking (bot h√§ngt nicht)

---

### 3. **KI/LLM BIBLIOTHEKEN**

#### `ollama==0.3.3`
- **WAS:** Python Client f√ºr Ollama Server
- **WARUM:** Vereinfacht Kommunikation mit lokalen LLMs
- **OHNE DAS:** M√ºsstest HTTP-Requests manuell machen
- **VERWENDET IN:** Optional (wir benutzen aiohttp direkt)
- **MODELS:** llama3.2:3b, nomic-embed-text

---

### 4. **VEKTORDATENBANK BIBLIOTHEKEN**

#### `chromadb==0.4.18`
- **WAS:** Vektordatenbank f√ºr semantische Suche
- **WARUM:** Speichert Text als Vektoren f√ºr √§hnlichkeitsbasierte Suche
- **OHNE DAS:** Keine intelligente Suche, nur exakte Textsuche
- **VERWENDET IN:** `vector_store.py`
- **ERSTELLT:** `chroma_db/` Ordner

#### `sentence-transformers==2.2.2`
- **WAS:** Erstellt Text-Embeddings (Text zu Vektoren)
- **WARUM:** Fallback falls Ollama Embeddings nicht funktionieren
- **OHNE DAS:** Nur Ollama Embeddings verf√ºgbar
- **VERWENDET IN:** `vector_store.py` (Fallback)

---

### 5. **PDF VERARBEITUNG BIBLIOTHEKEN**

#### `PyPDF2==3.0.1`
- **WAS:** PDF-Dateien lesen und Text extrahieren
- **WARUM:** Macht PDF-Inhalte f√ºr Computer lesbar
- **OHNE DAS:** PDFs bleiben unlesbar f√ºr Bot
- **VERWENDET IN:** `pdf_parser.py`
- **LIMITATION:** Funktioniert nur bei Text-PDFs (nicht Scans)

#### `pdf2image==1.16.3`
- **WAS:** Konvertiert PDF-Seiten zu Bildern
- **WARUM:** F√ºr OCR bei gescannten PDFs
- **OHNE DAS:** Gescannte PDFs nicht lesbar
- **VERWENDET IN:** `pdf_parser.py`, `handlers.py` (Screenshots)
- **BEN√ñTIGT:** Poppler (System-Tool)

#### `pytesseract==0.3.10`
- **WAS:** OCR (Optical Character Recognition) - Text aus Bildern extrahieren
- **WARUM:** Gescannte PDF-Seiten haben Text als Bilder
- **OHNE DAS:** Scans bleiben unlesbar
- **VERWENDET IN:** `pdf_parser.py`
- **BEN√ñTIGT:** Tesseract (System-Tool)

#### `Pillow==10.1.0`
- **WAS:** Bildverarbeitung (Python Imaging Library)
- **WARUM:** Bilder bearbeiten (zuschneiden, konvertieren)
- **OHNE DAS:** Screenshot-Funktion funktioniert nicht
- **VERWENDET IN:** `handlers.py` (Screenshots)

---

### 6. **HILFSBIBLIOTHEKEN**

#### `requests==2.31.0`
- **WAS:** Einfache HTTP-Requests
- **WARUM:** Synchrone HTTP-Calls (f√ºr Embeddings)
- **OHNE DAS:** Embedding-Requests komplizierter
- **VERWENDET IN:** `vector_store.py`
- **UNTERSCHIED zu aiohttp:** Synchron vs Asynchron

#### `numpy==1.24.3`
- **WAS:** Numerische Berechnungen (Arrays, Matrizen)
- **WARUM:** ChromaDB und Embeddings verwenden Arrays
- **OHNE DAS:** Vektordatenbank funktioniert nicht
- **AUTOMATISCH:** Wird von ChromaDB mitinstalliert

---

## üõ†Ô∏è SYSTEM-TOOLS (m√ºssen separat installiert werden)

### **Ollama**
- **WAS:** Lokaler LLM Server
- **INSTALLATION:** [ollama.ai](https://ollama.ai)
- **WARUM:** L√§uft KI-Modelle lokal (kein Internet n√∂tig)
- **KOMMANDOS:**
  ```bash
  ollama pull llama3.2:3b        # LLM Model herunterladen
  ollama pull nomic-embed-text   # Embedding Model herunterladen
  ollama serve                   # Server starten
  ```

### **Docker**
- **WAS:** Containerisierung-Platform
- **INSTALLATION:** [docker.com](https://docker.com)
- **WARUM:** Bot in isolierter Umgebung laufen lassen
- **KOMMANDOS:**
  ```bash
  docker build -t mein-bot .     # Image erstellen
  docker run -p 8000:8000 mein-bot  # Container starten
  ```

### **Tesseract OCR**
- **WAS:** Open-Source OCR Engine
- **INSTALLATION:** 
  - Windows: https://github.com/UB-Mannheim/tesseract/wiki
  - Linux: `sudo apt install tesseract-ocr`
- **WARUM:** pytesseract braucht diese Engine
- **SPRACHEN:** Deutsch, Englisch, etc.

### **Poppler**
- **WAS:** PDF-Utilities
- **INSTALLATION:**
  - Windows: https://blog.alivate.com.au/poppler-windows/
  - Linux: `sudo apt install poppler-utils`
- **WARUM:** pdf2image braucht diese Tools
- **ENTH√ÑLT:** pdftoppm, pdfinfo, etc.

---

## üìÅ PROJEKT-ORDNER ERKL√ÑRT

### **`chroma_db/`**
- **WAS:** Vektordatenbank-Speicher
- **INHALT:** 
  - Embeddings (Text als Vektoren)
  - Index-Dateien
  - Metadaten
- **WARUM:** Persistente Speicherung der Vektoren
- **KANN GEL√ñSCHT WERDEN:** Ja, wird neu erstellt
- **GR√ñ√üE:** W√§chst mit Anzahl der PDFs

### **`__pycache__/`**
- **WAS:** Python Bytecode Cache
- **INHALT:** Kompilierte .pyc Dateien
- **WARUM:** Python-Optimierung (schnellere Starts)
- **AUTOMATISCH:** Wird von Python erstellt
- **KANN GEL√ñSCHT WERDEN:** Ja, wird automatisch neu erstellt
- **IM GIT:** Sollte ignoriert werden (.gitignore)

### **`.dockerignore`**
- **WAS:** Docker Ignore-Datei
- **ZWECK:** Definiert was NICHT ins Docker Image kommt
- **ENTH√ÑLT:** __pycache__, .git, chroma_db, etc.
- **WARUM:** Kleinere, sauberere Docker Images

### **`Dockerfile`**
- **WAS:** Docker Image Bauanleitung
- **ZWECK:** Definiert wie Bot-Container erstellt wird
- **ENTH√ÑLT:** 
  - Python Installation
  - System-Dependencies (Tesseract, Poppler)
  - Python-Bibliotheken
  - App-Code

### **`requirements.txt`**
- **WAS:** Python Dependencies Liste
- **ZWECK:** Definiert alle ben√∂tigten Bibliotheken
- **FORMAT:** `bibliothek==version`
- **INSTALLATION:** `pip install -r requirements.txt`

---

## üîÑ WIE ALLES ZUSAMMENARBEITET

### **DATENFLUSS:**
1. **PDF** ‚Üí pdf_parser.py ‚Üí **Text-Chunks**
2. **Text-Chunks** ‚Üí vector_store.py ‚Üí **Embeddings**
3. **Embeddings** ‚Üí ChromaDB ‚Üí **Vektordatenbank**
4. **User-Frage** ‚Üí Telegram ‚Üí bot.py ‚Üí handlers.py
5. **Frage** ‚Üí vector_store.py ‚Üí **√Ñhnliche Chunks**
6. **Chunks** ‚Üí llm_client.py ‚Üí Ollama ‚Üí **Antwort**
7. **Antwort** ‚Üí handlers.py ‚Üí Telegram ‚Üí **User**

### **ARCHITEKTUR:**
```
User (Telegram) 
    ‚Üï 
ngrok (Tunnel)
    ‚Üï
FastAPI (bot.py) 
    ‚Üï
Handlers (handlers.py)
    ‚Üï
Vector Search (vector_store.py) ‚Üê ChromaDB
    ‚Üï
LLM Client (llm_client.py) 
    ‚Üï
Ollama Server (llama3.2:3b)
```

---

## üöÄ WARUM DIESE TECHNOLOGIE-AUSWAHL?

### **Warum Python?**
- Einfache Syntax
- Viele KI-Bibliotheken
- Gro√üe Community

### **Warum FastAPI?**
- Schnell und modern
- Automatische Dokumentation
- Async Support

### **Warum ChromaDB?**
- Einfach zu verwenden
- Gute Performance
- Lokaler Betrieb

### **Warum Ollama?**
- Lokale KI (keine Cloud)
- Einfache Installation
- Viele Modelle

### **Warum Docker?**
- Reproduzierbare Umgebung
- Einfache Deployment
- Isolation

---

## üéì LERNEMPFEHLUNGEN

### **Reihenfolge zum Lernen:**
1. **Python Grundlagen** (if/for/functions)
2. **HTTP/APIs verstehen** (GET/POST Requests)
3. **Async/Await Konzept** (asynchrone Programmierung)
4. **Vector Databases** (Embeddings, Similarity Search)
5. **LLM Concepts** (Prompts, Tokens, Context)
6. **Docker Basics** (Images, Containers)

### **Praktische √úbungen:**
1. Test einzelne Funktionen (test_pdf_extraction.py)
2. √Ñndere Prompts in llm_client.py
3. Experimentiere mit chunk_size
4. Probiere andere Ollama Models
5. Baue eigene Handler

---

## üÜò TROUBLESHOOTING

### **H√§ufige Probleme:**
1. **"Ollama not found"** ‚Üí Ollama installieren und starten
2. **"Tesseract not found"** ‚Üí Tesseract installieren
3. **"pdf2image error"** ‚Üí Poppler installieren
4. **"Port already in use"** ‚Üí Anderen Port verwenden
5. **"ChromaDB error"** ‚Üí chroma_db/ Ordner l√∂schen

### **Debug-Tipps:**
1. Pr√ºfe Logs in Terminal
2. Teste einzelne Komponenten
3. Verwende print() Statements
4. Pr√ºfe Environment Variables
5. Teste mit curl

---

**üéØ FAZIT:** Jede Bibliothek hat einen spezifischen Zweck. Zusammen ergeben sie einen vollst√§ndigen, intelligenten Telegram Bot der PDFs verstehen und Fragen beantworten kann!