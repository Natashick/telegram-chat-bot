## Update — 2025-10-22 — Telegram RAG Bot (ChromaDB + Ollama)

### Ziel / Purpose
- Stabiler Telegram‑Bot mit semantischer Suche über Automotive‑Cybersecurity‑PDFs (OCR, Embeddings, lokales LLM), reproduzierbar via Docker.

### Ausgangslage
- Tech-Stack: FastAPI, python-telegram-bot, ChromaDB, PyPDF2 + pdf2image + pytesseract, Ollama.
- Dockerfile vorhanden; compose optional. PDFs ursprünglich im Projekt‑Root.

### Probleme, die heute auftraten
- Hohe Systemlast: gleichzeitige OCR, Embeddings und LLM‑Generierung überlasteten den Laptop.
- Webhook‑Fehler: Telegram meldete „Wrong response … 404“ (veralteter ngrok‑Link) → Updates stauten sich.
- PDFs wurden nicht gefunden: Bot suchte in `/app/pdfs`, Dateien lagen im Projekt‑Root.
- Keine Suchtreffer: Embeddings waren leer → ChromaDB bekam keine sinnvollen Vektoren.

### Durchgeführte Maßnahmen
- Docker‑Ressourcen limitiert: `--cpus=1.0`, `--memory=2g`; Antwort‑LLM auf leichtes Modell gestellt: `OLLAMA_MODEL=llama3.2:1b`.
- Für lokale Tests auf Polling gewechselt (Webhook gelöscht), Health‑Check OK.
- PDFs in lokale Mappe `./pdfs` verschoben; Volume‑Mapping: `-v ./pdfs:/app/pdfs:ro`, `PDF_DIR=/app/pdfs`.
- Auf dem Host Modelle geprüft/gezogen: `llama3.2:1b`, `nomic-embed-text`.
- Embeddings‑Diagnostik aus dem Container:
  - POST `/api/embeddings` mit `{"model":"nomic-embed-text","input":["hello"]}` → 200, aber leeres Feld `embedding`.
  - POST mit `{"model":"nomic-embed-text","prompt":"hello"}` → 200 + valider Vektor (Dim ≈ 768).

### Root‑Cause
- Die aktuell laufende Ollama‑Embeddings‑API erwartet `prompt` (nicht `input`).
- `vector_store.py` verwendet `input` (Batch & per‑Item) → leere Embeddings → keine Treffer bei der Suche.

### Geplanter Fix (Code)
- `vector_store.py`: Embedding‑Client kompatibel machen.
  - Erst Batch mit `input` versuchen; wenn leer/inkonsistent → per‑Item mit `prompt`.
  - Ergebnisse validieren (nicht‑leer, plausible Dimension), leere Antworten verwerfen.
- Danach: ChromaDB leeren/rebuilden, PDFs neu indexieren, `/status` prüfen.

### Aktueller Status
- Bot im Container läuft (Polling), Health OK.
- PDFs werden erkannt; Indexierung startet.
- Suche liefert noch keine Ergebnisse, bis der Embedding‑Fix ausgerollt und neu indexiert wurde.

### Nächste Schritte
1) Fix in `vector_store.py` implementieren (prompt/input‑Fallback).
2) Container neu starten; `chroma_db` säubern oder per `/status`‑Workflow neu aufbauen.
3) Validierung: `/status` (Indexierte Chunks > 0), Beispiel‑Frage → Antwort mit Kontext.
4) Optional: OCR‑DPI reduzieren und `batch_size` verkleinern für schwächere Hardware.

### Referenz‑Kommandos (vereinfacht)
```bash
# Start (Polling, Ressourcen limitiert)
docker run --rm -d --name mein-bot \
  --cpus=1.0 --memory=2g \
  -e TELEGRAM_TOKEN=***REDACTED*** \
  -e OLLAMA_URL=http://host.docker.internal:11434 \
  -e OLLAMA_MODEL=llama3.2:1b \
  -e OLLAMA_EMBED_MODEL=nomic-embed-text \
  -e OCR_CONCURRENCY=1 \
  -e PDF_DIR=/app/pdfs \
  -v ./pdfs:/app/pdfs:ro \
  -v ./chroma_db:/app/chroma_db \
  -p 8000:8000 \
  mein-bot

# Health
curl http://localhost:8000/health

# Embeddings (Diagnose)
# input (leer in dieser Ollama‑Version)
POST /api/embeddings {"model":"nomic-embed-text","input":["hello"]}
# prompt (liefert Vektor)
POST /api/embeddings {"model":"nomic-embed-text","prompt":"hello"}
```

—
Erstellt am 2025‑10‑22. Zusammenfassung der heutigen Sitzung und Erkenntnisse.
