# üß† OLLAMA SERVER HOSTING OPTIONEN

## PROBLEM
Dein Bot braucht einen Ollama Server mit llama3.2:3b Model.
Cloud-Provider unterst√ºtzen Ollama meist nicht direkt.

## L√ñSUNGEN

### 1. üè† **HYBRID: Bot in Cloud, Ollama lokal (EINFACHSTE L√ñSUNG)**

```
‚úÖ Bot l√§uft in Railway/Heroku (√∂ffentlich zug√§nglich)
‚úÖ Ollama l√§uft weiter auf deinem PC
‚úÖ Bot verbindet sich √ºber Internet zu deinem Ollama
```

#### Setup:
```bash
# 1. Auf deinem PC: Ollama mit √∂ffentlichem Zugang starten
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# 2. Router/Firewall Port 11434 √∂ffnen (Port Forwarding)
# 3. Deine √∂ffentliche IP herausfinden: https://whatismyipaddress.com
# 4. Bot OLLAMA_URL setzen: http://DEINE-EXTERNE-IP:11434
```

**Vorteile:**
- ‚úÖ Kostenlos
- ‚úÖ Volle Kontrolle √ºber Ollama
- ‚úÖ Kein Upload von Models n√∂tig

**Nachteile:**
- ‚ö†Ô∏è PC muss immer laufen
- ‚ö†Ô∏è Router-Konfiguration n√∂tig

### 2. ‚òÅÔ∏è **VPS MIT OLLAMA (BESTE LANGZEIT-L√ñSUNG)**

```
Miete VPS (Virtual Private Server) und installiere Ollama dort
```

#### Anbieter-Optionen:

**A) DigitalOcean Droplet**
```
Basic Droplet: $12/Monat (2GB RAM, 1 vCPU)
Premium Droplet: $24/Monat (4GB RAM, 2 vCPU) - EMPFOHLEN f√ºr llama3.2:3b
```

**B) Vultr**
```
Regular Instance: $12/Monat (4GB RAM, 2 vCPU)
High Performance: $24/Monat (8GB RAM, 3 vCPU)
```

**C) Linode**
```
Nanode: $5/Monat (1GB RAM) - ZU KLEIN
Shared CPU: $12/Monat (2GB RAM) - MINIMAL
Dedicated CPU: $24/Monat (4GB RAM) - EMPFOHLEN
```

#### VPS Setup:
```bash
# 1. Ubuntu 22.04 VPS erstellen
# 2. SSH verbinden
ssh root@VPS-IP

# 3. Ollama installieren
curl -fsSL https://ollama.ai/install.sh | sh

# 4. Model herunterladen
ollama pull llama3.2:3b
ollama pull nomic-embed-text

# 5. Ollama als Service starten
sudo systemctl enable ollama
sudo systemctl start ollama

# 6. Firewall Port √∂ffnen
sudo ufw allow 11434

# 7. Ollama f√ºr externe Verbindungen konfigurieren
echo 'Environment="OLLAMA_HOST=0.0.0.0:11434"' | sudo tee /etc/systemd/system/ollama.service.d/override.conf
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### 3. üîÑ **ALTERNATIVE: OPENAI API (TEURER ABER EINFACHER)**

```
Ersetze Ollama komplett durch OpenAI API
```

#### Code-√Ñnderung in llm_client.py:
```python
import openai

async def ask_openai(question, context):
    client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    response = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context."},
            {"role": "user", "content": f"Context: {context}\n\nQuestion: {question}"}
        ],
        max_tokens=500,
        temperature=0.3
    )
    
    return response.choices[0].message.content
```

**Kosten:**
- GPT-3.5-turbo: ~$0.002 pro 1K tokens
- F√ºr 1000 Fragen/Monat: ~$5-10

## EMPFEHLUNG F√úR DICH

### ü•á **PHASE 1: HYBRID SETUP (SOFORT)**
```
1. Bot ‚Üí Railway (kostenlos)
2. Ollama ‚Üí dein PC + Port Forwarding
3. Funktioniert sofort, keine Zusatzkosten
```

### ü•à **PHASE 2: VPS UPGRADE (SP√ÑTER)**
```
1. Bot ‚Üí Railway/DigitalOcean
2. Ollama ‚Üí VPS ($12-24/Monat)
3. Professioneller, 24/7 verf√ºgbar
```

### ü•â **PHASE 3: COMMERCIAL (BEI VIELEN USERN)**
```
1. Bot ‚Üí Professioneller Cloud Service
2. LLM ‚Üí OpenAI API oder Azure OpenAI
3. Skaliert automatisch
```